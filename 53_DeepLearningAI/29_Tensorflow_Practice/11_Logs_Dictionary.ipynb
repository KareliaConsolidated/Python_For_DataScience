{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    # Print the loss after every second batch in the training set\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 2 == 0:\n",
    "            print(f\"After batch {batch}, the loss is {logs['loss']:7.2f}\")\n",
    "            \n",
    "    # Print the loss after each batch in the test set\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "\n",
    "    # Print the loss and mean absolute error after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n",
    "    \n",
    "    # Notify the user when prediction has finished on each batch\n",
    "    def on_predict_batch_end(self,batch, logs=None):\n",
    "        print(\"Finished prediction on batch {}!\".format(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 0, the loss is 29867.67\n",
      "After batch 2, the loss is 29948.75\n",
      "Epoch 0: Average loss is 29676.37, mean absolute error is  154.12.\n",
      "After batch 0, the loss is 30945.99\n",
      "After batch 2, the loss is 30142.36\n",
      "Epoch 1: Average loss is 29548.61, mean absolute error is  153.73.\n",
      "After batch 0, the loss is 29542.86\n",
      "After batch 2, the loss is 28733.87\n",
      "Epoch 2: Average loss is 29356.59, mean absolute error is  153.14.\n",
      "After batch 0, the loss is 29369.72\n",
      "After batch 2, the loss is 29444.87\n",
      "Epoch 3: Average loss is 29089.23, mean absolute error is  152.31.\n",
      "After batch 0, the loss is 27370.87\n",
      "After batch 2, the loss is 29436.39\n",
      "Epoch 4: Average loss is 28721.51, mean absolute error is  151.17.\n",
      "After batch 0, the loss is 32752.90\n",
      "After batch 2, the loss is 28186.91\n",
      "Epoch 5: Average loss is 28252.03, mean absolute error is  149.65.\n",
      "After batch 0, the loss is 26233.89\n",
      "After batch 2, the loss is 27606.98\n",
      "Epoch 6: Average loss is 27625.21, mean absolute error is  147.63.\n",
      "After batch 0, the loss is 30181.72\n",
      "After batch 2, the loss is 27437.69\n",
      "Epoch 7: Average loss is 26828.70, mean absolute error is  145.04.\n",
      "After batch 0, the loss is 29226.82\n",
      "After batch 2, the loss is 25421.56\n",
      "Epoch 8: Average loss is 25821.82, mean absolute error is  141.71.\n",
      "After batch 0, the loss is 27778.27\n",
      "After batch 2, the loss is 24801.91\n",
      "Epoch 9: Average loss is 24642.07, mean absolute error is  137.70.\n",
      "After batch 0, the loss is 24455.41\n",
      "After batch 2, the loss is 23087.04\n",
      "Epoch 10: Average loss is 23087.38, mean absolute error is  132.46.\n",
      "After batch 0, the loss is 19738.72\n",
      "After batch 2, the loss is 21335.33\n",
      "Epoch 11: Average loss is 21414.71, mean absolute error is  126.20.\n",
      "After batch 0, the loss is 21331.38\n",
      "After batch 2, the loss is 19530.85\n",
      "Epoch 12: Average loss is 19398.95, mean absolute error is  118.83.\n",
      "After batch 0, the loss is 15928.92\n",
      "After batch 2, the loss is 17330.33\n",
      "Epoch 13: Average loss is 17130.10, mean absolute error is  109.70.\n",
      "After batch 0, the loss is 14279.86\n",
      "After batch 2, the loss is 15366.14\n",
      "Epoch 14: Average loss is 14789.92, mean absolute error is  100.15.\n",
      "After batch 0, the loss is 16279.43\n",
      "After batch 2, the loss is 13048.17\n",
      "Epoch 15: Average loss is 12567.75, mean absolute error is   90.32.\n",
      "After batch 0, the loss is 10673.78\n",
      "After batch 2, the loss is 11295.07\n",
      "Epoch 16: Average loss is 10447.22, mean absolute error is   80.51.\n",
      "After batch 0, the loss is 9014.08\n",
      "After batch 2, the loss is 8480.11\n",
      "Epoch 17: Average loss is 8847.87, mean absolute error is   73.99.\n",
      "After batch 0, the loss is 5147.10\n",
      "After batch 2, the loss is 7864.36\n",
      "Epoch 18: Average loss is 7736.33, mean absolute error is   68.89.\n",
      "After batch 0, the loss is 6674.40\n",
      "After batch 2, the loss is 6698.72\n",
      "Epoch 19: Average loss is 6594.22, mean absolute error is   63.36.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=20,\n",
    "                    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 16312.49.\n",
      "\n",
      " After batch 1, the loss is 13018.31.\n",
      "\n",
      " After batch 2, the loss is 15713.27.\n",
      "\n",
      " After batch 3, the loss is 13781.72.\n",
      "\n",
      " After batch 4, the loss is 14140.08.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n",
    "                            callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction on batch 0!\n",
      "Finished prediction on batch 1!\n",
      "Finished prediction on batch 2!\n",
      "Finished prediction on batch 3!\n",
      "Finished prediction on batch 4!\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "\n",
    "model_pred = model.predict(test_data, batch_size=10,\n",
    "                           callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application -- LRS Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate scheduler function\n",
    "lr_schedule = [\n",
    "    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n",
    "]\n",
    "\n",
    "def get_new_epoch_lr(epoch, lr):\n",
    "    epoch_in_sch = [i for i in range(len(lr_schedule)) if lr_schedule[i][0] == int(epoch)]\n",
    "    if len(epoch_in_sch)> 0:\n",
    "        return lr_schedule[epoch_in_sch[0]][1]\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom callback\n",
    "\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, new_lr):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        # Add the new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "              raise ValueError('Error: Optimizer does not have a learning rate.')\n",
    "                \n",
    "        # Get the current learning rate\n",
    "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        # Call the auxillary function to get the scheduled learning rate for the current epoch\n",
    "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "\n",
    "        # Set the learning rate to the scheduled learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the same model as before\n",
    "\n",
    "new_model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "new_model.compile(loss='mse',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 0 is   0.001\n",
      "Learning rate for epoch 1 is   0.001\n",
      "Learning rate for epoch 2 is   0.001\n",
      "Learning rate for epoch 3 is   0.001\n",
      "Learning rate for epoch 4 is   0.030\n",
      "Learning rate for epoch 5 is   0.030\n",
      "Learning rate for epoch 6 is   0.030\n",
      "Learning rate for epoch 7 is   0.020\n",
      "Learning rate for epoch 8 is   0.020\n",
      "Learning rate for epoch 9 is   0.020\n",
      "Learning rate for epoch 10 is   0.020\n",
      "Learning rate for epoch 11 is   0.005\n",
      "Learning rate for epoch 12 is   0.005\n",
      "Learning rate for epoch 13 is   0.005\n",
      "Learning rate for epoch 14 is   0.005\n",
      "Learning rate for epoch 15 is   0.007\n",
      "Learning rate for epoch 16 is   0.007\n",
      "Learning rate for epoch 17 is   0.007\n",
      "Learning rate for epoch 18 is   0.007\n",
      "Learning rate for epoch 19 is   0.007\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with our learning rate scheduler callback\n",
    "\n",
    "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
    "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
